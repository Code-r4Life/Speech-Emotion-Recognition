# ğŸ™ï¸ Speech Emotion Recognition using Deep Learning
### **Author: Shinjan Saha**

This project implements a **Speech Emotion Recognition (SER)** system that detects human emotions from voice recordings using advanced audio signal processing and a deep CNN model.

The system analyzes speech patterns and classifies emotions into multiple categories, enabling emotion-aware AI applications.

---

# ğŸ“Œ 1. Project Overview

This repository provides a complete pipeline for:

- ğŸ§ Audio preprocessing & augmentation  
- ğŸ”Š Feature extraction from speech signals  
- ğŸ§  Deep learning model training  
- ğŸ“Š Evaluation & visualization  
- ğŸ”® Real-time emotion prediction  

The model classifies speech into **7 emotions**:

ğŸ˜¡ Angry  
ğŸ¤¢ Disgust  
ğŸ˜¨ Fear  
ğŸ˜Š Happy  
ğŸ˜ Neutral  
ğŸ˜¢ Sad  
ğŸ˜² Surprise  

---

# ğŸ§  2. Dataset Used

## ğŸ§ RAVDESS Emotional Speech Dataset
- Professional emotional speech recordings
- Multiple actors & emotional intensities

## ğŸ§ TESS Emotional Speech Dataset
- Clear emotional speech recordings
- High-quality pronunciation & tone variations

---

# âš™ï¸ 3. Tech Stack

### **Languages & Libraries**

- Python  
- TensorFlow / Keras  
- Librosa (audio processing)  
- NumPy & Pandas  
- Scikit-learn  
- Matplotlib & Seaborn  

---

# ğŸ”Š 4. Audio Processing Pipeline

## ğŸ”¹ Step 1 â€” Preprocessing
- Load audio files
- Trim silence & normalize signals
- Standardize sampling rate

## ğŸ”¹ Step 2 â€” Data Augmentation
To improve robustness and generalization:

âœ” Noise Injection  
âœ” Time Stretching  
âœ” Pitch Shifting  
âœ” Signal Shifting  

## ğŸ”¹ Step 3 â€” Feature Extraction

Extracted features:

- Zero Crossing Rate (ZCR)
- Chroma STFT
- MFCC (Mel Frequency Cepstral Coefficients)
- Root Mean Square Energy (RMS)
- Mel Spectrogram

---

# ğŸ—ï¸ 5. Model Architecture

A deep **1D Convolutional Neural Network** designed for sequential audio features.

ğŸ“· Model Summary  

[__results___files\cnn_arch.png]


### ğŸ”¹ Architecture Details

- Multiple Conv1D feature extraction blocks  
- Batch Normalization for stability  
- MaxPooling for dimensionality reduction  
- Dropout layers to prevent overfitting  
- Dense layers for classification  
- Softmax output layer (7 emotions)

**Total Parameters:** 2.67M  
**Trainable Parameters:** 2.66M  

---

# ğŸ“Š 6. Training Strategy

- EarlyStopping to prevent overfitting  
- ReduceLROnPlateau for adaptive learning  
- ModelCheckpoint for best model saving  
- StandardScaler for feature normalization  

---

# ğŸ“ˆ 7. Model Performance

## ğŸ”¹ Classification Report

[__results___files\classification_report.png]


**Overall Accuracy:** **92%**  
**Macro Avg F1 Score:** **0.92**

### Key Observations:
- Neutral emotion has highest recall (0.98)
- Strong precision across fear & angry classes
- Balanced performance across all emotions

---

## ğŸ”¹ Training & Validation Performance

[__results___files\__results___45_1.png]


**Insights:**
- Smooth convergence
- Minimal overfitting
- Stable validation accuracy (~91â€“92%)

---

## ğŸ”¹ Confusion Matrix

[__results___files\__results___48_0.png]


**Insights:**
- Strong diagonal dominance â†’ accurate predictions  
- Minor confusion between happy & neutral  
- Fear & angry classification highly reliable  

---

# ğŸ§ª 8. Results Summary

âœ” Test Accuracy: **92%**  
âœ” Balanced performance across classes  
âœ” Robust predictions with augmented audio  
âœ” Generalizes well across speakers  

---

# ğŸ’¾ 9. Model Export & Inference

The saved pipeline includes:

- trained CNN model  
- scaler  
- label encoder  
- feature extractor  

Saved as: `emotion_preprocessing.joblib`


## ğŸ”® Predict Emotion from New Audio

```python
prediction = get_predictions("audio.wav", emotion_preprocess)
print(prediction)
```

---

# ğŸ“ 10. Project Structure

```
Speech-Emotion-Recognition/
â”‚
â”œâ”€â”€ model
â”‚ â””â”€â”€best_emotion_model.keras
â”œâ”€â”€ metrics
â”‚ â”œâ”€â”€ __results___45_1.png
â”‚ â”œâ”€â”€ __results___48_0.png
â”‚ â”œâ”€â”€ classification_report.png
â”‚ â””â”€â”€cnn_arch.png
â”œâ”€â”€ __results___files
â”œâ”€â”€ emotion_path.csv
â”œâ”€â”€ features.csv
â”œâ”€â”€ speech-emotion-recognition.ipynb
â”œâ”€â”€ emotion_preprocessing.joblib
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ› ï¸ 11. Installation

```
git clone https://github.com/Code-r4Life/Speech-Emotion-Recognition.git
cd Speech-Emotion-Recognition
pip install -r requirements.txt
```

---

# ğŸŒ 12. Real-World Applications

- ğŸ§ Voice assistants & conversational AI
- ğŸ§  Mental health & stress monitoring
- ğŸ“ Customer sentiment analysis
- ğŸ® Emotion-aware gaming
- ğŸ“š Smart education platforms

---

# ğŸ“¬ Interested in a Similar Project?

I build smart, ML-integrated applications and responsive web platforms. Letâ€™s build something powerful together!

ğŸ“§ shinjansaha00@gmail.com

ğŸ”— [LinkedIn Profile](https://www.linkedin.com/in/shinjan-saha-1bb744319/)